{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a253a50-5613-4376-9633-01b2df44a962",
   "metadata": {},
   "source": [
    "# Fonction calcul métriques WER CER à partir des jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52a8dc98-ec79-47d0-853f-d47c3b0ae963",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from jiwer import wer, cer\n",
    "\n",
    "def evaluate_asr_predictions(jsonl_path: str):\n",
    "    \"\"\"\n",
    "    Calcule WER et CER en comparant `raw_text` et `model_prediction` dans un fichier JSONL.\n",
    "    Retourne un dictionnaire avec les métriques globales.\n",
    "    \"\"\"\n",
    "    jsonl_path = Path(jsonl_path)\n",
    "    if not jsonl_path.exists():\n",
    "        raise FileNotFoundError(f\"{jsonl_path} n'existe pas\")\n",
    "\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "\n",
    "    with open(jsonl_path, \"r\", encoding=\"utf-8\") as fin:\n",
    "        for line in fin:\n",
    "            item = json.loads(line)\n",
    "            ref = item.get(\"raw_text\", \"\").strip()\n",
    "            hyp = item.get(\"model_prediction\", \"\").strip()\n",
    "            if ref and hyp:\n",
    "                references.append(ref)\n",
    "                hypotheses.append(hyp)\n",
    "\n",
    "    if not references:\n",
    "        raise ValueError(\"Aucune référence valide trouvée dans le fichier.\")\n",
    "\n",
    "    overall_wer = wer(references, hypotheses)\n",
    "    overall_cer = cer(references, hypotheses)\n",
    "\n",
    "    print(f\"✅ Fichier évalué : {jsonl_path}\")\n",
    "    print(f\"Nombre de phrases évaluées : {len(references)}\")\n",
    "    print(f\"WER (Word Error Rate) : {overall_wer:.3f}\")\n",
    "    print(f\"CER (Character Error Rate) : {overall_cer:.3f}\")\n",
    "\n",
    "    return {\n",
    "        \"num_sentences\": len(references),\n",
    "        \"WER\": overall_wer,\n",
    "        \"CER\": overall_cer\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5c5e34-8bf0-411d-8229-c73c2bc39ea1",
   "metadata": {},
   "source": [
    "# Fast Whisper 71K audios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ead8080-784f-4509-ad42-b25d11e1423d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fichier évalué : metadata_with_predictions_small.jsonl\n",
      "Nombre de phrases évaluées : 70900\n",
      "WER (Word Error Rate) : 0.195\n",
      "CER (Character Error Rate) : 0.087\n",
      "✅ Fichier évalué : metadata_with_predictions_medium.jsonl\n",
      "Nombre de phrases évaluées : 70848\n",
      "WER (Word Error Rate) : 0.167\n",
      "CER (Character Error Rate) : 0.077\n",
      "✅ Fichier évalué : metadata_with_predictions_large.jsonl\n",
      "Nombre de phrases évaluées : 70931\n",
      "WER (Word Error Rate) : 0.146\n",
      "CER (Character Error Rate) : 0.066\n",
      "Small: {'num_sentences': 70900, 'WER': 0.1952426863288178, 'CER': 0.08723724917371074}\n",
      "Medium: {'num_sentences': 70848, 'WER': 0.16669175207505274, 'CER': 0.07656037507156996}\n",
      "Large: {'num_sentences': 70931, 'WER': 0.1464902743806228, 'CER': 0.06649615495507732}\n"
     ]
    }
   ],
   "source": [
    "metrics_small = evaluate_asr_predictions(\"metadata_with_predictions_small.jsonl\")\n",
    "metrics_medium = evaluate_asr_predictions(\"metadata_with_predictions_medium.jsonl\")\n",
    "metrics_large = evaluate_asr_predictions(\"metadata_with_predictions_large.jsonl\")\n",
    "\n",
    "print(\"Small:\", metrics_small)\n",
    "print(\"Medium:\", metrics_medium)\n",
    "print(\"Large:\", metrics_large)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25631d4a-208a-4f39-8933-9210aeb81c70",
   "metadata": {},
   "source": [
    "# Whisper 1K audios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22e90a84-2890-4a52-b297-2b67de05161e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from jiwer import wer, cer\n",
    "\n",
    "def evaluate_asr_predictions(jsonl_path: str, model_size: str = \"small\"):\n",
    "    \"\"\"\n",
    "    Calcule WER et CER en comparant `raw_text` et `model_prediction_<size>` dans un fichier JSONL.\n",
    "    \n",
    "    Paramètres :\n",
    "        jsonl_path : str - chemin vers le fichier JSONL\n",
    "        model_size : str - \"small\", \"medium\" ou \"large\" (par défaut \"small\")\n",
    "        \n",
    "    Retourne un dictionnaire avec les métriques globales.\n",
    "    \"\"\"\n",
    "    jsonl_path = Path(jsonl_path)\n",
    "    if not jsonl_path.exists():\n",
    "        raise FileNotFoundError(f\"{jsonl_path} n'existe pas\")\n",
    "\n",
    "    valid_sizes = {\"small\", \"medium\", \"large\"}\n",
    "    if model_size not in valid_sizes:\n",
    "        raise ValueError(f\"model_size doit être l'un de {valid_sizes}\")\n",
    "\n",
    "    prediction_key = f\"model_prediction_{model_size}\"\n",
    "\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "\n",
    "    with open(jsonl_path, \"r\", encoding=\"utf-8\") as fin:\n",
    "        for line in fin:\n",
    "            item = json.loads(line)\n",
    "            ref = item.get(\"raw_text\", \"\").strip()\n",
    "            hyp = item.get(prediction_key, \"\").strip()\n",
    "            if ref and hyp:\n",
    "                references.append(ref)\n",
    "                hypotheses.append(hyp)\n",
    "\n",
    "    if not references:\n",
    "        raise ValueError(\"Aucune référence valide trouvée dans le fichier.\")\n",
    "\n",
    "    overall_wer = wer(references, hypotheses)\n",
    "    overall_cer = cer(references, hypotheses)\n",
    "\n",
    "    print(f\"✅ Fichier évalué : {jsonl_path}\")\n",
    "    print(f\"Nombre de phrases évaluées : {len(references)}\")\n",
    "    print(f\"Modèle évalué : {model_size}\")\n",
    "    print(f\"WER (Word Error Rate) : {overall_wer:.3f}\")\n",
    "    print(f\"CER (Character Error Rate) : {overall_cer:.3f}\")\n",
    "\n",
    "    return {\n",
    "        \"num_sentences\": len(references),\n",
    "        \"model_size\": model_size,\n",
    "        \"WER\": overall_wer,\n",
    "        \"CER\": overall_cer\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4226aa65-6489-4481-9873-7b6c95b83061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fichier évalué : data/voxpopuli_fr_train/train_metadata_full_small_speed_benchmark.jsonl\n",
      "Nombre de phrases évaluées : 960\n",
      "Modèle évalué : small\n",
      "WER (Word Error Rate) : 0.191\n",
      "CER (Character Error Rate) : 0.079\n",
      "✅ Fichier évalué : data/voxpopuli_fr_train/train_metadata_full_medium_speed_benchmark.jsonl\n",
      "Nombre de phrases évaluées : 958\n",
      "Modèle évalué : medium\n",
      "WER (Word Error Rate) : 0.162\n",
      "CER (Character Error Rate) : 0.069\n",
      "✅ Fichier évalué : data/voxpopuli_fr_train/train_metadata_full_large_speed_benchmark.jsonl\n",
      "Nombre de phrases évaluées : 961\n",
      "Modèle évalué : large\n",
      "WER (Word Error Rate) : 0.142\n",
      "CER (Character Error Rate) : 0.061\n",
      "Small: {'num_sentences': 960, 'model_size': 'small', 'WER': 0.19091903719912473, 'CER': 0.07902004398680396}\n",
      "Medium: {'num_sentences': 958, 'model_size': 'medium', 'WER': 0.16197623514696685, 'CER': 0.06852739533401246}\n",
      "Large: {'num_sentences': 961, 'model_size': 'large', 'WER': 0.14169787765293385, 'CER': 0.06093666180121512}\n"
     ]
    }
   ],
   "source": [
    "metrics_small = evaluate_asr_predictions(\"data/voxpopuli_fr_train/train_metadata_full_small_speed_benchmark.jsonl\",\"small\")\n",
    "metrics_medium = evaluate_asr_predictions(\"data/voxpopuli_fr_train/train_metadata_full_medium_speed_benchmark.jsonl\",\"medium\")\n",
    "metrics_large = evaluate_asr_predictions(\"data/voxpopuli_fr_train/train_metadata_full_large_speed_benchmark.jsonl\",\"large\")\n",
    "\n",
    "print(\"Small:\", metrics_small)\n",
    "print(\"Medium:\", metrics_medium)\n",
    "print(\"Large:\", metrics_large)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9126a2-eed1-451d-b2e2-8ee926b091ec",
   "metadata": {},
   "source": [
    "# Whisper fine tuned French Bofenuang 1K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea3f5bca-4bbe-4e8b-8d5c-20ab4d450826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fichier évalué : sample_1000_whisper_french.jsonl\n",
      "Nombre de phrases évaluées : 960\n",
      "WER (Word Error Rate) : 0.091\n",
      "CER (Character Error Rate) : 0.045\n"
     ]
    }
   ],
   "source": [
    "metrics_large_bofenghuang_fr = evaluate_asr_predictions(\"sample_1000_whisper_french.jsonl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (projet9)",
   "language": "python",
   "name": "projet9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44309a84-77f4-4234-a610-e70732a92e69",
   "metadata": {},
   "source": [
    "# Apllication C2translate sur modèle base whisper fine tuned français"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533e7bfa-064b-4d40-9db8-eb90c01e9d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://huggingface.co/brandenkmurray/faster-whisper-large-v3-french-distil-dec16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4201ca-925d-4f6d-b763-ed960f430bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ctranslate2 transformers\n",
    "\n",
    "git lfs install\n",
    "git clone https://huggingface.co/BrunoHays/whisper-large-v3-french-illuin\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a534ce7-dc0a-422b-af15-ada076aea471",
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo apt install ffmpeg libavcodec-dev libavformat-dev libavdevice-dev libavutil-dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b28ef8-7b7c-431d-8245-e6815ddb5c8d",
   "metadata": {},
   "source": [
    "# Download large distilled 16 + ctranslate2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5260749-ae88-415a-954f-85a30d2782ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2141624470.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mpython -c \"from huggingface_hub import snapshot_download; snapshot_download(repo_id='bofenghuang/whisper-large-v3-french-distil-dec16', local_dir='./models/whisper-large-v3-french-distil-dec16', allow_patterns='ctranslate2/*')\"\u001b[39m\n              ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "python -c \"from huggingface_hub import snapshot_download; snapshot_download(repo_id='bofenghuang/whisper-large-v3-french-distil-dec16', local_dir='./models/whisper-large-v3-french-distil-dec16', allow_patterns='ctranslate2/*')\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58199321-c08b-4c6b-8454-95a90160b7cf",
   "metadata": {},
   "source": [
    "# Download large distilled 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330973e7-1df8-4e26-b34e-56c0b97da16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Télécharger tous les fichiers du modèle PyTorch natif\n",
    "snapshot_download(\n",
    "    repo_id='bofenghuang/whisper-large-v3-french-distil-dec16',\n",
    "    local_dir='./models/whisper-large-v3-french-distil-dec16',\n",
    "    allow_patterns='*'   # Télécharge tout le repo\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50632ff2-840f-4b4e-b951-0527f071c3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00s -> 5.52s] Nous allons construire des radiotélescopes géants, comme celui-ci qui est mon préféré.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from faster_whisper import WhisperModel\n",
    "\n",
    "# Load model\n",
    "model = WhisperModel(\"./models/whisper-large-v3-french-distil-dec16/ctranslate2\", device=\"cpu\", compute_type=\"int8\")  # Run on GPU with FP16\n",
    "\n",
    "# Example audio\n",
    "dataset = load_dataset(\"bofenghuang/asr-dummy\", \"fr\", split=\"test\")\n",
    "sample = dataset[0][\"audio\"][\"array\"].astype(\"float32\")\n",
    "\n",
    "segments, info = model.transcribe(sample, beam_size=5, language=\"fr\")\n",
    "\n",
    "for segment in segments:\n",
    "    print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9515a852-134e-43cd-af8b-e308c73172e4",
   "metadata": {},
   "source": [
    "# Modèle Whisper large distilled 32 -> 16 + Ctranslate2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7e84f9-035b-473f-b10a-91795cb4f511",
   "metadata": {},
   "source": [
    "# Int 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f9926cb-5f29-42a6-bdc3-bc0511ade812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langue détectée : fr\n",
      "[0.00s -> 7.20s] Vous devez donner suite à tous vos engagements et nous vous les rappellerons jusqu'au dernier jour de cette législature.\n"
     ]
    }
   ],
   "source": [
    "from faster_whisper import WhisperModel\n",
    "\n",
    "# Chemin vers le modèle CTranslate2\n",
    "model_path = \"./models/whisper-large-v3-french-distil-dec16-ct2/ctranslate2\"\n",
    "\n",
    "# Instanciation du modèle sur CPU avec quantification int8\n",
    "model = WhisperModel(\n",
    "    model_path,\n",
    "    device=\"cpu\",        # CPU uniquement\n",
    "    compute_type=\"int8\"  # accélération quantifiée\n",
    ")\n",
    "\n",
    "# Transcription d'un fichier audio\n",
    "audio_file = \"test_20201111_17_21_20_8.wav\"\n",
    "segments, info = model.transcribe(\n",
    "    audio_file,\n",
    "    beam_size=5,\n",
    "    language=\"fr\",\n",
    "    condition_on_previous_text=False\n",
    ")\n",
    "\n",
    "# Affichage de la langue détectée\n",
    "print(\"Langue détectée :\", info.language)\n",
    "\n",
    "# Affichage du texte segment par segment\n",
    "for segment in segments:\n",
    "    print(f\"[{segment.start:.2f}s -> {segment.end:.2f}s] {segment.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc937cf3-abbb-4ebb-9cbb-f404f915e4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langue détectée : fr\n",
      "[0.00s -> 30.00s] Il y a 5 ans, l'humanité a traversé l'une des plus grandes épreuves sanitaires de notre siècle. Face à la pandémie de Covid dixneuf, ensemble, nous avons tenu bon, ensemble nous avons inventé et partagé des solutions, notamment des vaccins, et ensemble nous avons inventé de nouvelles règles, que ce soit sur le partage de la propriété intellectuelle ou pour accélérer les processus industriels. Nous avons ainsi démontré que le multilatéralisme, loin d'être un concept vain.\n",
      "[30.00s -> 60.00s] Était la méthode la plus efficace pour protéger nos populations. Aujourd'hui encore, cette unité nous guide face aux menaces sanitaires nouvelles. Je pense au Mpox, au choléra, à toutes les guerres et conflits, à toutes les victimes et à ceux qui luttent chaque jour pour les soigner, et je veux dire ici notre solidarité, notre reconnaissance pour l'ensemble des soignants, des humanitaires qui sont mobilisés dans ces combats. La présentation aujourd'hui de l'accord international sur les pandémies.\n",
      "[60.00s -> 71.00s] La pandémie marque une avancée historique. Négocié en 3 ans, cet accord est une victoire pour l'avenir, c'est une victoire pour nos populations qui seront désormais mieux protégées contre les pandémies.\n",
      "[71.00s -> 101.00s] La France s'est mobilisée sans relâche afin de porter une vision qui est aujourd'hui ancrée dans ce traité celle d'une réponse plus efficace, plus équitable, qui traite tout le monde à la même enseigne car on ne peut éradiquer une pandémie si on laisse un foyer actif et en face des responsabilités plus claires et mieux partagées afin aussi que les pays contribuent à prévenir les prochaines pandémies. Et au fond, ce qu'il y a derrière cet accord, c'est cette volonté d'avoir des mécanismes d'alerte, partagés, transparents.\n",
      "[101.00s -> 129.80s] Plus efficace que ce que nous avons connu lors du COVID et c'est de réconcilier les concepts d'efficacité et d'équité. Il faut aller le plus vite possible, innover le plus vite possible, mais il faut tout de suite diffuser le plus largement possible ces innovations, car les 2 sont inséparables. L'accord pandémie consacre également à la nécessité de mieux partager les informations scientifiques pour répondre aux pandémies et développer les traitements, les vaccins, les tests, ce qui est une exigence fondamentale pour notre sécurité collective.\n",
      "[131.00s -> 139.56s] Mes chers amis, il est essentiel de le redire ici, alors que certains pensent pouvoir se passer de la science et voudraient en quelque sorte réduire au silence la recherche.\n",
      "[139.56s -> 155.36s]  Non seulement cela nuirait à notre santé à tous, mais c'est en 1er lieu la population de ceux qui font en quelque sorte ce retrait, ce recul, qui sera en danger face à l'émergence de nouveaux pathogènes qu'ils ne verraient ainsi pas revenir.\n",
      "[155.36s -> 185.36s] Heureusement, nous avons la force de la sécurité que nous offre l'Organisation mondiale de la santé, son travail normatif, le nouvel appui de cet accord, ce bouclier mondial contre les pandémies qui vient d'être créé et que vous avez souhaité. Et le monde vous en remercie. À tous les chercheurs qui souhaitent poursuivre librement leurs travaux, je veux ici le redire, comme nous l'avons affirmé avec la présidente de la Commission européenne, l'Europe est prête à vous accueillir, parce que nous pensons que c'est bon pour vous, pour leur travail.\n",
      "[185.36s -> 215.36s] En Europe, mais aussi pour la recherche à l'échelle mondiale. Parce que la question n'est pas si mais quand arrivera la prochaine pandémie. Nous avons ancré avec cet accord, pour la 1ère fois, dans le droit international, l'approche une seule santé, que nous défendons depuis maintenant plus de 4 ans. C'est l'idée selon laquelle la santé humaine, la santé animale, l'environnement sont indissociables. À ce sujet, je veux ici que nous continuions à travailler de manière très concrète sur des questions qui touchent nos populations au quotidien.\n",
      "[215.36s -> 243.76s] Je pense aux épidémies de chikungunia ou de dengue qui touchent en particulier nombre de territoires français, je pense aux pollutions de l'air, du sol, de l'eau, notamment par des microplastiques, c'est pourquoi je souhaite que nous nous retrouvions à Lyon, le 3 novembre prochain, à l'occasion de la Journée Mondiale Une Seule Santé, pour un One Health Summit, qui nous permettra d'avancer sur ces questions de manière déterminée avec l'ensemble des acteurs publics et privés de bonne volonté.\n",
      "[245.36s -> 275.36s] La pandémie de COVID 19 nous a rappelé une valeur essentielle rien ne serait possible sans celles et ceux qui soignent, et l'accord reconnaît l'importance de cet enjeu investir dans la formation, garantir des conditions de travail décentes, préserver la sécurité ou la santé mentale de nos soignants, atténuer les distorsions créées par l'émigration des personnels de santé qualifiés et j'en passe. L'inauguration de l'académie de l'OMS à Lyon en décembre dernier nous offre un instrument de 1er plan au service du renforcement massif et durable.\n",
      "[275.36s -> 305.36s] Des personnels de santé, et ce tout au long de leur carrière et partout dans le monde. Enfin, je souhaite ici souligner à propos de cet accord un dernier point majeur il permet de renforcer la production mondiale, c'est à dire sur tous les continents, des produits nécessaires pour sauver les vies humaines en période de pandémie. C'est un sujet sur lequel la France s'est engagée de longue date à vos côtés, cher Tedros, comme dans le hub de production de vaccins ARN messagers en Afrique du Sud.\n",
      "[305.36s -> 319.36s] Qui est saine désormais dans des dizaines de pays. C'est aussi cet accélérateur de production de vaccins en Afrique, doté de 1000000000 de dollars, que nous avons lancé en juin dernier à Paris avec Gavi. Ce combat, nous ne devons pas l'arrêter.\n",
      "[319.36s -> 335.36s]  Chers amis, cet accord marque la victoire du multilatéralisme sur des logiques de repli, la victoire de la coopération sur l'indifférence. C'est aussi la confirmation que la vie humaine, la santé doivent toujours l'emporter sur les intérêts particuliers.\n",
      "[335.36s -> 349.74s] Je veux ici remercier très sincèrement tous ceux qui se sont mobilisés pour ce texte, notamment les coprésidentes de l'Organe international de négociation, Mme Precius Matosso, pour l'Afrique du Sud, et notre ambassadrice pour la santé, Anne Claire Amproux.\n",
      "[349.74s -> 360.74s]  En adoptant cet accord, nous portons une conviction forte protéger la santé des populations, c'est assurer la paix, la sécurité, la stabilité et donc l'avenir de nos sociétés.\n",
      "[360.74s -> 374.74s] Chers amis, chers Tedros, vous pouvez compter sur le soutien déterminé de la France dans cette période complexe de réforme de nos organisations de la santé mondiale, afin que celle-ci en sorte plus forte et prête à affronter les défis de demain.\n",
      "[374.74s -> 386.80s]  Plein soutien donc à cet accord, avec ma gratitude pour tout le travail fait, et surtout l'ambition renouvelée pour la suite. Je vous souhaite une très bonne Assemblée mondiale pour la santé. Merci à toutes et tous.\n"
     ]
    }
   ],
   "source": [
    "from faster_whisper import WhisperModel\n",
    "\n",
    "# Chemin vers le modèle CTranslate2\n",
    "model_path = \"./models/whisper-large-v3-french-distil-dec16-ct2/ctranslate2\"\n",
    "\n",
    "# Instanciation du modèle sur CPU avec quantification int8\n",
    "model = WhisperModel(\n",
    "    model_path,\n",
    "    device=\"cpu\",        # CPU uniquement\n",
    "    compute_type=\"int8\"  # accélération quantifiée\n",
    ")\n",
    "\n",
    "# Transcription d'un fichier audio\n",
    "audio_file = \"macron.mp3\"\n",
    "segments, info = model.transcribe(\n",
    "    audio_file,\n",
    "    beam_size=5,\n",
    "    language=\"fr\",\n",
    "    condition_on_previous_text=False\n",
    ")\n",
    "\n",
    "# Affichage de la langue détectée\n",
    "print(\"Langue détectée :\", info.language)\n",
    "\n",
    "# Affichage du texte segment par segment\n",
    "for segment in segments:\n",
    "    print(f\"[{segment.start:.2f}s -> {segment.end:.2f}s] {segment.text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2aaf2b-abd0-4658-b49d-8406009e84e9",
   "metadata": {},
   "source": [
    "# Float 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4a0abd7-6c07-4885-b092-12b4ea97e0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langue détectée : fr\n",
      "[0.00s -> 7.14s] Vous devez donner suite à tous vos engagements et nous vous les rappellerons jusqu'au dernier jour de cette législature.\n"
     ]
    }
   ],
   "source": [
    "from faster_whisper import WhisperModel\n",
    "\n",
    "# Chemin vers le modèle CTranslate2\n",
    "model_path = \"./models/whisper-large-v3-french-distil-dec16-ct2/ctranslate2\"\n",
    "\n",
    "# Instanciation du modèle sur CPU avec quantification int8\n",
    "model = WhisperModel(\n",
    "    model_path,\n",
    "    device=\"cpu\",        # CPU uniquement\n",
    "    compute_type=\"float32\"  # accélération quantifiée\n",
    ")\n",
    "\n",
    "# Transcription d'un fichier audio\n",
    "audio_file = \"test_20201111_17_21_20_8.wav\"\n",
    "segments, info = model.transcribe(\n",
    "    audio_file,\n",
    "    beam_size=5,\n",
    "    language=\"fr\",\n",
    "    condition_on_previous_text=False\n",
    ")\n",
    "\n",
    "# Affichage de la langue détectée\n",
    "print(\"Langue détectée :\", info.language)\n",
    "\n",
    "# Affichage du texte segment par segment\n",
    "for segment in segments:\n",
    "    print(f\"[{segment.start:.2f}s -> {segment.end:.2f}s] {segment.text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60ade6f-4257-414f-b5c0-c6eabb7bb532",
   "metadata": {},
   "source": [
    "# Whisper large french distilled 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a9974f-9e93-4b43-b9cd-2fceb4321996",
   "metadata": {},
   "source": [
    "# Int8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e04532e4-f4b6-4115-a345-0c89d2efec0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2107465/1281350102.py:17: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  model_int8 = torch.quantization.quantize_dynamic(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription : Vous devez donner suite à tous vos engagements et nous vous les rappellerons jusqu'au dernier jour de cette législature.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\n",
    "import torch\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "\n",
    "# Chemin du modèle\n",
    "model_name_or_path = \"./models/whisper-large-v3-french-distil-dec16\"\n",
    "\n",
    "# Charger le processeur\n",
    "processor = AutoProcessor.from_pretrained(model_name_or_path)\n",
    "\n",
    "# Charger le modèle en float32\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(model_name_or_path)\n",
    "model.eval()  # mode évaluation\n",
    "\n",
    "# Quantization dynamique en int8 pour CPU\n",
    "model_int8 = torch.quantization.quantize_dynamic(\n",
    "    model,                # modèle original\n",
    "    {torch.nn.Linear},    # couches à quantifier\n",
    "    dtype=torch.qint8\n",
    ")\n",
    "\n",
    "# Charger l'audio\n",
    "audio_file = \"test.wav\"\n",
    "audio, sr = sf.read(audio_file)\n",
    "if sr != 16000:\n",
    "    audio = librosa.resample(audio, orig_sr=sr, target_sr=16000)\n",
    "\n",
    "# Préparer les entrées\n",
    "inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\")\n",
    "input_features = inputs[\"input_features\"]  # Pas d'attention_mask nécessaire\n",
    "\n",
    "# Transcription\n",
    "with torch.no_grad():\n",
    "    generated_ids = model_int8.generate(input_features)\n",
    "    transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(\"Transcription :\", transcription)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd40289-3ce7-4367-b14f-63e3f7abc2f4",
   "metadata": {},
   "source": [
    "# Whisper large french"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b716fc1-8404-4fce-8623-f269ce349db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c8032a8d03e499ea755e922f878b6fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 22 files:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5abfd67a7ffe43eab3ff706dcd1feec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d7856b6e83b4e938a2573f1ad93b768",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b242a723a19444d9392bf79571b120a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "072ac60bcba24494a19c74711e8595a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ctranslate2/model.bin:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bc7325081404c39b188ef2314893e50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/340 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e1814b2684a4cb68b522faf00be959e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e278016eb36418ba1c9ad088e17c2cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "whisper_fr_eval_long_form.png:   0%|          | 0.00/234k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c7aae8d03ff4223a0ea54aa2423662e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocabulary.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8d02663a3764aaebee07a578e955bcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "whisper_fr_eval_short_form.png:   0%|          | 0.00/218k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9afcac199b874b5e99ea41a6a52b6955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8fe63df9a494fdb982967cd9babeaac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5662cad7c5d4aac8961781ce0881f56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b5972e94def45c293d51a71e0feacc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae1f3871438148e28440fb5f1b5462b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/340 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6463c35a93a5433b98cfb004dba8009e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cddfd59493b4506b611027bfd49fdf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ggml-model-q5_0.bin:   0%|          | 0.00/1.08G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cb66ca3dfbc4a43b3fc7dd689b31ba2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ggml-model.bin:   0%|          | 0.00/3.10G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9491a4afa3f643fa9e1fbe93245c9ae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f288621d6444a40a80f19f66f016584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "original_model.pt:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5601cc8ce5ac4cbd947d0549460a9598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb40585dc6f74fe08914191af3b0b59f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f6341b2c1834f3ca73db08596dbb52a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/home/projet9/projet9/notebooks/models/whisper-large-v3-french'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Téléchargement du modèle complet\n",
    "snapshot_download(\n",
    "    repo_id=\"bofenghuang/whisper-large-v3-french\",\n",
    "    local_dir=\"./models/whisper-large-v3-french\",\n",
    "    allow_patterns=[\"*\"]  # \"*\" pour tout télécharger\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4965ad09-0a50-416c-bef3-92282522a5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2107465/262084083.py:17: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  model_int8 = torch.quantization.quantize_dynamic(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription : Vous devez donner suite à tous vos engagements et nous vous les rappellerons jusqu'au dernier jour de cette législature.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\n",
    "import torch\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "\n",
    "# Chemin vers le modèle non distillé\n",
    "model_name_or_path = \"./models/whisper-large-v3-french\"\n",
    "\n",
    "# Charger le processeur\n",
    "processor = AutoProcessor.from_pretrained(model_name_or_path)\n",
    "\n",
    "# Charger le modèle en float32 et passer en mode évaluation\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(model_name_or_path)\n",
    "model.eval()\n",
    "\n",
    "# Quantization dynamique en int8 sur CPU\n",
    "model_int8 = torch.quantization.quantize_dynamic(\n",
    "    model,\n",
    "    {torch.nn.Linear},  # seules les couches Linear sont quantifiées\n",
    "    dtype=torch.qint8\n",
    ")\n",
    "\n",
    "# Chemin vers le fichier audio\n",
    "audio_file = \"test.wav\"\n",
    "\n",
    "# Charger l'audio\n",
    "audio, sr = sf.read(audio_file)\n",
    "if sr != 16000:\n",
    "    audio = librosa.resample(audio, orig_sr=sr, target_sr=16000)\n",
    "\n",
    "# Préparer les entrées (attention_mask non nécessaire pour Whisper)\n",
    "inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\")\n",
    "input_features = inputs[\"input_features\"]\n",
    "\n",
    "# Génération de la transcription\n",
    "with torch.no_grad():\n",
    "    generated_ids = model_int8.generate(input_features)\n",
    "    transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(\"Transcription :\", transcription)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4560a71e-157f-41b1-ab71-2fea0affef75",
   "metadata": {},
   "source": [
    "# Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e2e49d0-c025-41b2-b52a-f42955371e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Whisper Large V3 French (non distillé, dyn int8) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2112764/1941055802.py:24: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  model_int8 = torch.quantization.quantize_dynamic(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription : Vous devez donner suite à tous vos engagements et nous vous les rappellerons jusqu'au dernier jour de cette législature.\n",
      "Inference time: 5.18 s\n",
      "\n",
      "=== Whisper Large V3 French Distilled (dyn int8) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2112764/1941055802.py:48: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  model_int8 = torch.quantization.quantize_dynamic(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription : Vous devez donner suite à tous vos engagements et nous vous les rappellerons jusqu'au dernier jour de cette législature.\n",
      "Inference time: 3.21 s\n",
      "\n",
      "=== Whisper Large V3 French Distilled CTranslate2 (int8) ===\n",
      "Langue détectée : fr\n",
      "[0.00s -> 7.20s] Vous devez donner suite à tous vos engagements et nous vous les rappellerons jusqu'au dernier jour de cette législature.\n",
      "Inference time: 0.23 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\n",
    "from faster_whisper import WhisperModel\n",
    "\n",
    "# Chemin vers le fichier audio\n",
    "audio_file = \"test.wav\"\n",
    "\n",
    "# Charger l'audio\n",
    "audio, sr = sf.read(audio_file)\n",
    "if sr != 16000:\n",
    "    audio = librosa.resample(audio, orig_sr=sr, target_sr=16000)\n",
    "\n",
    "########################################\n",
    "# 1. Modèle non distillé (float32 -> dyn int8)\n",
    "########################################\n",
    "print(\"\\n=== Whisper Large V3 French (non distillé, dyn int8) ===\")\n",
    "model_name = \"./models/whisper-large-v3-french\"\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(model_name)\n",
    "model.eval()\n",
    "model_int8 = torch.quantization.quantize_dynamic(\n",
    "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "\n",
    "inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\")\n",
    "input_features = inputs[\"input_features\"]\n",
    "\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    generated_ids = model_int8.generate(input_features)\n",
    "    transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Transcription :\", transcription)\n",
    "print(f\"Inference time: {end_time - start_time:.2f} s\")\n",
    "\n",
    "########################################\n",
    "# 2. Modèle distillé (dyn int8)\n",
    "########################################\n",
    "print(\"\\n=== Whisper Large V3 French Distilled (dyn int8) ===\")\n",
    "model_name = \"./models/whisper-large-v3-french-distil-dec16\"\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(model_name)\n",
    "model.eval()\n",
    "model_int8 = torch.quantization.quantize_dynamic(\n",
    "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "\n",
    "inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\")\n",
    "input_features = inputs[\"input_features\"]\n",
    "\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    generated_ids = model_int8.generate(input_features)\n",
    "    transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Transcription :\", transcription)\n",
    "print(f\"Inference time: {end_time - start_time:.2f} s\")\n",
    "\n",
    "########################################\n",
    "# 3. Modèle distillé CTranslate2 (int8)\n",
    "########################################\n",
    "print(\"\\n=== Whisper Large V3 French Distilled CTranslate2 (int8) ===\")\n",
    "ct2_model_path = \"./models/whisper-large-v3-french-distil-dec16-ct2/ctranslate2\"\n",
    "model = WhisperModel(\n",
    "    ct2_model_path,\n",
    "    device=\"cpu\",\n",
    "    compute_type=\"int8\"\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "segments, info = model.transcribe(\n",
    "    audio_file,\n",
    "    beam_size=5,\n",
    "    language=\"fr\",\n",
    "    condition_on_previous_text=False\n",
    ")\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Langue détectée :\", info.language)\n",
    "for segment in segments:\n",
    "    print(f\"[{segment.start:.2f}s -> {segment.end:.2f}s] {segment.text}\")\n",
    "print(f\"Inference time: {end_time - start_time:.2f} s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f516af2-04d9-40ab-baac-0b556b83852e",
   "metadata": {},
   "source": [
    "# Benchmark temporel 10 fichiers OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5d134d8-dda6-41e8-834d-79fb22df3559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 10 fichiers choisis au hasard ===\n",
      "20170426-0900-PLENARY-14-fr_20170426-19:50:54_8.wav\n",
      "20141021-0900-PLENARY-10-fr_20141021-16:11:33_34.wav\n",
      "20090203-0900-PLENARY-4-fr_20090203-11:21:16_9.wav\n",
      "20170705-0900-PLENARY-17-fr_20170705-19:39:56_7.wav\n",
      "20131121-0900-PLENARY-16-fr_20131121-15:42:54_7.wav\n",
      "20090203-0900-PLENARY-4-fr_20090203-11:21:16_3.wav\n",
      "20180313-0900-PLENARY-4-fr_20180313-11:13:14_4.wav\n",
      "20131024-0900-PLENARY-3-fr_20131024-08:30:20_29.wav\n",
      "20180417-0900-PLENARY-4-fr_20180417-11:21:18_10.wav\n",
      "20170215-0900-PLENARY-16-fr_20170215-19:51:40_3.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2125024/206810654.py:34: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  model_non_int8 = torch.quantization.quantize_dynamic(\n",
      "/tmp/ipykernel_2125024/206810654.py:43: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  model_distil_int8 = torch.quantization.quantize_dynamic(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "? JSON récapitulatif sauvegardé dans 'benchmark_results.json'\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\n",
    "from faster_whisper import WhisperModel\n",
    "\n",
    "# === Nouvelle fonction : choisir 10 fichiers audio au hasard ===\n",
    "def get_random_voxpopuli_audios(base_dir=\"data/voxpopuli_fr_test/test_part_0\", n=10):\n",
    "    all_files = [f for f in os.listdir(base_dir) if f.endswith(\".wav\")]\n",
    "    return random.sample(all_files, min(n, len(all_files)))\n",
    "\n",
    "# Exemple : récupérer une liste de 10 fichiers\n",
    "base_dir = \"data/voxpopuli_fr_test/test_part_0\"\n",
    "random_audios = get_random_voxpopuli_audios(base_dir, n=10)\n",
    "print(\"\\n=== 10 fichiers choisis au hasard ===\")\n",
    "for f in random_audios:\n",
    "    print(f)\n",
    "\n",
    "# Dictionnaire pour stocker les résultats\n",
    "results = {}\n",
    "\n",
    "########################################\n",
    "# Charger les modèles une seule fois\n",
    "########################################\n",
    "# 1. Modèle non distillé (float32 -> dyn int8)\n",
    "model_name = \"./models/whisper-large-v3-french\"\n",
    "processor_non = AutoProcessor.from_pretrained(model_name)\n",
    "model_non = AutoModelForSpeechSeq2Seq.from_pretrained(model_name)\n",
    "model_non.eval()\n",
    "model_non_int8 = torch.quantization.quantize_dynamic(\n",
    "    model_non, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "\n",
    "# 2. Modèle distillé (dyn int8)\n",
    "model_name = \"./models/whisper-large-v3-french-distil-dec16\"\n",
    "processor_distil = AutoProcessor.from_pretrained(model_name)\n",
    "model_distil = AutoModelForSpeechSeq2Seq.from_pretrained(model_name)\n",
    "model_distil.eval()\n",
    "model_distil_int8 = torch.quantization.quantize_dynamic(\n",
    "    model_distil, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "\n",
    "# 3. Modèle distillé CTranslate2 (int8)\n",
    "ct2_model_path = \"./models/whisper-large-v3-french-distil-dec16-ct2/ctranslate2\"\n",
    "model_ct2 = WhisperModel(\n",
    "    ct2_model_path,\n",
    "    device=\"cpu\",\n",
    "    compute_type=\"int8\"\n",
    ")\n",
    "\n",
    "########################################\n",
    "# Boucle sur les 10 fichiers audio\n",
    "########################################\n",
    "for wav_file in random_audios:\n",
    "    audio_path = os.path.join(base_dir, wav_file)\n",
    "\n",
    "    # Charger l'audio\n",
    "    audio, sr = sf.read(audio_path)\n",
    "    if sr != 16000:\n",
    "        audio = librosa.resample(audio, orig_sr=sr, target_sr=16000)\n",
    "\n",
    "    file_results = {}\n",
    "\n",
    "    # ====== Modèle non distillé ======\n",
    "    inputs = processor_non(audio, sampling_rate=16000, return_tensors=\"pt\")\n",
    "    input_features = inputs[\"input_features\"]\n",
    "\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model_non_int8.generate(input_features)\n",
    "        transcription = processor_non.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    end_time = time.time()\n",
    "\n",
    "    file_results[\"whisper_large_non_distilled\"] = {\n",
    "        \"transcription\": transcription,\n",
    "        \"inference_time_s\": end_time - start_time\n",
    "    }\n",
    "\n",
    "    # ====== Modèle distillé ======\n",
    "    inputs = processor_distil(audio, sampling_rate=16000, return_tensors=\"pt\")\n",
    "    input_features = inputs[\"input_features\"]\n",
    "\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model_distil_int8.generate(input_features)\n",
    "        transcription = processor_distil.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    end_time = time.time()\n",
    "\n",
    "    file_results[\"whisper_large_distilled\"] = {\n",
    "        \"transcription\": transcription,\n",
    "        \"inference_time_s\": end_time - start_time\n",
    "    }\n",
    "\n",
    "    # ====== Modèle distillé CTranslate2 ======\n",
    "    start_time = time.time()\n",
    "    segments, info = model_ct2.transcribe(\n",
    "        audio_path,\n",
    "        beam_size=5,\n",
    "        language=\"fr\",\n",
    "        condition_on_previous_text=False\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    ct2_transcription = \" \".join([s.text for s in segments])\n",
    "    file_results[\"whisper_large_distilled_ct2\"] = {\n",
    "        \"transcription\": ct2_transcription,\n",
    "        \"language_detected\": info.language,\n",
    "        \"inference_time_s\": end_time - start_time\n",
    "    }\n",
    "\n",
    "    # Sauvegarder les résultats pour ce fichier\n",
    "    results[wav_file] = file_results\n",
    "\n",
    "# Sauvegarder le JSON récapitulatif\n",
    "with open(\"benchmark_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"\\n? JSON récapitulatif sauvegardé dans 'benchmark_results.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90a89b0-0aa8-428b-b711-78b84f8add45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaea974-d8aa-4b80-8700-2213424253e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a03a1534-40cc-4898-a6ea-509f7bcfcd37",
   "metadata": {},
   "source": [
    "# Benchmark temporel + precision Large / Large distilled / Large distilled CT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03e3b59c-e070-41b6-9d65-778d4288b052",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2460959/1897776081.py:38: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  model_non_int8 = torch.quantization.quantize_dynamic(\n",
      "/tmp/ipykernel_2460959/1897776081.py:47: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  model_distil_int8 = torch.quantization.quantize_dynamic(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "? JSON enrichi avec inference_time, elapsed_time, RTF, WER, CER et somme totale des elapsed_time\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\n",
    "from faster_whisper import WhisperModel\n",
    "from jiwer import wer, cer\n",
    "\n",
    "# === Nouvelle fonction : choisir n fichiers audio au hasard ===\n",
    "def get_random_voxpopuli_audios(base_dir=\"data/voxpopuli_fr_test/test_part_0\", n=100):\n",
    "    all_files = [f for f in os.listdir(base_dir) if f.endswith(\".wav\")]\n",
    "    return random.sample(all_files, min(n, len(all_files)))\n",
    "\n",
    "# Charger le mapping id -> raw_text depuis le TSV\n",
    "tsv_path = \"data/voxpopuli_fr_test/asr_test.tsv\"\n",
    "df = pd.read_csv(tsv_path, sep=\"\\t\")\n",
    "id2raw = dict(zip(df[\"id\"], df[\"raw_text\"]))\n",
    "\n",
    "# Exemple : récupérer une liste\n",
    "base_dir = \"data/voxpopuli_fr_test/test_part_0\"\n",
    "random_audios = get_random_voxpopuli_audios(base_dir, n=100)\n",
    "\n",
    "results = {}\n",
    "\n",
    "########################################\n",
    "# Charger les modèles une seule fois\n",
    "########################################\n",
    "# 1. Modèle non distillé\n",
    "model_name = \"./models/whisper-large-v3-french\"\n",
    "processor_non = AutoProcessor.from_pretrained(model_name)\n",
    "model_non = AutoModelForSpeechSeq2Seq.from_pretrained(model_name)\n",
    "model_non.eval()\n",
    "model_non_int8 = torch.quantization.quantize_dynamic(\n",
    "    model_non, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "\n",
    "# 2. Modèle distillé\n",
    "model_name = \"./models/whisper-large-v3-french-distil-dec16\"\n",
    "processor_distil = AutoProcessor.from_pretrained(model_name)\n",
    "model_distil = AutoModelForSpeechSeq2Seq.from_pretrained(model_name)\n",
    "model_distil.eval()\n",
    "model_distil_int8 = torch.quantization.quantize_dynamic(\n",
    "    model_distil, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "\n",
    "# 3. Modèle distillé CTranslate2\n",
    "ct2_model_path = \"./models/whisper-large-v3-french-distil-dec16-ct2/ctranslate2\"\n",
    "model_ct2 = WhisperModel(\n",
    "    ct2_model_path,\n",
    "    device=\"cpu\",\n",
    "    compute_type=\"int8\"\n",
    ")\n",
    "\n",
    "########################################\n",
    "# Boucle sur les fichiers audio\n",
    "########################################\n",
    "for wav_file in random_audios:\n",
    "    audio_path = os.path.join(base_dir, wav_file)\n",
    "\n",
    "    # Charger l'audio\n",
    "    audio, sr = sf.read(audio_path)\n",
    "    if sr != 16000:\n",
    "        audio = librosa.resample(audio, orig_sr=sr, target_sr=16000)\n",
    "\n",
    "    # Calcul de la durée de l'audio (secondes)\n",
    "    duration_s = len(audio) / 16000.0\n",
    "\n",
    "    # Récupérer la référence (raw_text)\n",
    "    file_id = wav_file.replace(\".wav\", \"\")\n",
    "    raw_text = id2raw.get(file_id, \"\")\n",
    "\n",
    "    # Stocker au même niveau que duration_s\n",
    "    file_results = {\n",
    "        \"duration_s\": duration_s,\n",
    "        \"raw_text\": raw_text\n",
    "    }\n",
    "\n",
    "    # ====== Modèle non distillé ======\n",
    "    global_start = time.time()\n",
    "    inputs = processor_non(audio, sampling_rate=16000, return_tensors=\"pt\")\n",
    "    input_features = inputs[\"input_features\"]\n",
    "\n",
    "    inf_start = time.time()\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model_non_int8.generate(input_features)\n",
    "        transcription = processor_non.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    inf_end = time.time()\n",
    "    global_end = time.time()\n",
    "\n",
    "    inference_time = inf_end - inf_start\n",
    "    elapsed_time = global_end - global_start\n",
    "    rtf = inference_time / duration_s\n",
    "    file_results[\"whisper_large_non_distilled\"] = {\n",
    "        \"transcription\": transcription,\n",
    "        \"inference_time_s\": inference_time,\n",
    "        \"elapsed_time_s\": elapsed_time,\n",
    "        \"real_time_factor\": rtf\n",
    "    }\n",
    "\n",
    "    # ====== Modèle distillé ======\n",
    "    global_start = time.time()\n",
    "    inputs = processor_distil(audio, sampling_rate=16000, return_tensors=\"pt\")\n",
    "    input_features = inputs[\"input_features\"]\n",
    "\n",
    "    inf_start = time.time()\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model_distil_int8.generate(input_features)\n",
    "        transcription = processor_distil.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    inf_end = time.time()\n",
    "    global_end = time.time()\n",
    "\n",
    "    inference_time = inf_end - inf_start\n",
    "    elapsed_time = global_end - global_start\n",
    "    rtf = inference_time / duration_s\n",
    "    file_results[\"whisper_large_distilled\"] = {\n",
    "        \"transcription\": transcription,\n",
    "        \"inference_time_s\": inference_time,\n",
    "        \"elapsed_time_s\": elapsed_time,\n",
    "        \"real_time_factor\": rtf\n",
    "    }\n",
    "\n",
    "    # ====== Modèle distillé CTranslate2 ======\n",
    "    global_start = time.time()\n",
    "    inf_start = time.time()\n",
    "    segments, info = model_ct2.transcribe(\n",
    "        audio_path,\n",
    "        beam_size=5,\n",
    "        language=\"fr\",\n",
    "        condition_on_previous_text=False\n",
    "    )\n",
    "    inf_end = time.time()\n",
    "    global_end = time.time()\n",
    "\n",
    "    ct2_transcription = \" \".join([s.text for s in segments])\n",
    "    inference_time = inf_end - inf_start\n",
    "    elapsed_time = global_end - global_start\n",
    "    rtf = inference_time / duration_s\n",
    "    file_results[\"whisper_large_distilled_ct2\"] = {\n",
    "        \"transcription\": ct2_transcription,\n",
    "        \"language_detected\": info.language,\n",
    "        \"inference_time_s\": inference_time,\n",
    "        \"elapsed_time_s\": elapsed_time,\n",
    "        \"real_time_factor\": rtf\n",
    "    }\n",
    "\n",
    "    # Sauvegarder les résultats\n",
    "    results[wav_file] = file_results\n",
    "\n",
    "########################################\n",
    "# Calcul du WER, CER, temps moyen, RTF\n",
    "########################################\n",
    "report = {}\n",
    "for model_key in [\"whisper_large_non_distilled\", \"whisper_large_distilled\", \"whisper_large_distilled_ct2\"]:\n",
    "    wers, cers, times, elapsed_times, rtfs = [], [], [], [], []\n",
    "    for wav_file, res in results.items():\n",
    "        file_id = wav_file.replace(\".wav\", \"\")\n",
    "        if file_id not in id2raw:\n",
    "            continue\n",
    "        ref = str(id2raw[file_id]).lower().strip()\n",
    "        hyp = str(res[model_key][\"transcription\"]).lower().strip()\n",
    "        wers.append(wer(ref, hyp))\n",
    "        cers.append(cer(ref, hyp))\n",
    "        times.append(res[model_key][\"inference_time_s\"])\n",
    "        elapsed_times.append(res[model_key][\"elapsed_time_s\"])\n",
    "        rtfs.append(res[model_key][\"real_time_factor\"])\n",
    "    report[model_key] = {\n",
    "        \"WER\": float(np.mean(wers)) if wers else None,\n",
    "        \"CER\": float(np.mean(cers)) if cers else None,\n",
    "        \"avg_inference_time_s\": float(np.mean(times)) if times else None,\n",
    "        \"avg_elapsed_time_s\": float(np.mean(elapsed_times)) if elapsed_times else None,\n",
    "        \"sum_elapsed_time_s\": float(np.sum(elapsed_times)) if elapsed_times else None,  # ? Somme ajoutée\n",
    "        \"avg_real_time_factor\": float(np.mean(rtfs)) if rtfs else None\n",
    "    }\n",
    "\n",
    "# Sauvegarde\n",
    "final_output = {\"results\": results, \"report\": report}\n",
    "\n",
    "with open(\"benchmark_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(final_output, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"\\n? JSON enrichi avec inference_time, elapsed_time, RTF, WER, CER et somme totale des elapsed_time\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da331b94-6e72-4787-bef8-292960a3cf08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (projet9)",
   "language": "python",
   "name": "projet9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

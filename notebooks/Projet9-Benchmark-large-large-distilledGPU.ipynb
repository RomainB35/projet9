{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb5738e0-33b0-44e2-9d04-bbf5ac12d40c",
   "metadata": {},
   "source": [
    "# Benchmark whisper large fine tuned / whisper large fine tuned distilled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d51cf1c-bcea-4dcd-9a3f-f4f6680923bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶️ Utilisation du device : cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transcription:   0%|                                             | 0/10000 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Transcription:  44%|█████████████▏                | 4393/10000 [2:30:23<3:07:55,  2.01s/it]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import librosa\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\n",
    "from tqdm import tqdm  \n",
    "\n",
    "# ============================================\n",
    "# Config device\n",
    "# ============================================\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"▶️ Utilisation du device : {device}\")\n",
    "\n",
    "# === Charger TSV du subset ===\n",
    "tsv_path = \"data/subset10K/validated_subset.tsv\"\n",
    "clips_dir = \"data/subset10K/clips\"\n",
    "df = pd.read_csv(tsv_path, sep=\"\\t\")\n",
    "\n",
    "# === Utiliser tout le subset ===\n",
    "subset = df\n",
    "\n",
    "# === Charger les modèles ===\n",
    "model_name_non = \"./models/whisper-large-v3-french\"\n",
    "processor_non = AutoProcessor.from_pretrained(model_name_non)\n",
    "model_non = AutoModelForSpeechSeq2Seq.from_pretrained(model_name_non).to(device)\n",
    "model_non.eval()\n",
    "\n",
    "model_name_distil = \"./models/whisper-large-v3-french-distil-dec16\"\n",
    "processor_distil = AutoProcessor.from_pretrained(model_name_distil)\n",
    "model_distil = AutoModelForSpeechSeq2Seq.from_pretrained(model_name_distil).to(device)\n",
    "model_distil.eval()\n",
    "\n",
    "# === Fichier JSONL pour écrire au fur et à mesure ===\n",
    "output_dir = \"results\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file = os.path.join(output_dir, \"benchmark_large_only.jsonl\")\n",
    "\n",
    "# ============================================\n",
    "# Boucle sur tous les fichiers audio avec tqdm\n",
    "# ============================================\n",
    "for idx, row in enumerate(tqdm(subset.itertuples(), total=len(subset), desc=\"Transcription\")):\n",
    "    mp3_file = row.path\n",
    "    sentence = str(row.sentence)\n",
    "    audio_path = os.path.join(clips_dir, mp3_file)\n",
    "\n",
    "    if not os.path.exists(audio_path):\n",
    "        tqdm.write(f\"⚠️ Fichier introuvable : {audio_path}, ignoré.\")\n",
    "        continue\n",
    "\n",
    "    # Charger audio (mp3 -> PCM 16kHz mono float32)\n",
    "    audio, sr = librosa.load(audio_path, sr=16000, mono=True)\n",
    "    duration_s = len(audio) / 16000.0\n",
    "\n",
    "    file_results = {\"audio_file\": mp3_file, \"duration_s\": duration_s, \"raw_text\": sentence}\n",
    "\n",
    "    # ====== Large HF ======\n",
    "    start_global = time.time()\n",
    "    inputs = processor_non(audio, sampling_rate=16000, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    start_inf = time.time()\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model_non.generate(inputs[\"input_features\"])\n",
    "        transcription = processor_non.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    end_inf = time.time()\n",
    "    end_global = time.time()\n",
    "\n",
    "    file_results[\"whisper_large_non_distilled\"] = {\n",
    "        \"transcription\": transcription,\n",
    "        \"inference_time_s\": end_inf - start_inf,\n",
    "        \"elapsed_time_s\": end_global - start_global,\n",
    "        \"real_time_factor\": (end_inf - start_inf) / duration_s\n",
    "    }\n",
    "\n",
    "    # ====== Large-distilled HF ======\n",
    "    start_global = time.time()\n",
    "    inputs = processor_distil(audio, sampling_rate=16000, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    start_inf = time.time()\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model_distil.generate(inputs[\"input_features\"])\n",
    "        transcription = processor_distil.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    end_inf = time.time()\n",
    "    end_global = time.time()\n",
    "\n",
    "    file_results[\"whisper_large_distilled\"] = {\n",
    "        \"transcription\": transcription,\n",
    "        \"inference_time_s\": end_inf - start_inf,\n",
    "        \"elapsed_time_s\": end_global - start_global,\n",
    "        \"real_time_factor\": (end_inf - start_inf) / duration_s\n",
    "    }\n",
    "\n",
    "    # ====== Écriture immédiate dans JSONL ======\n",
    "    with open(output_file, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(file_results, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"✅ Benchmark terminé. Résultats sauvegardés dans {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

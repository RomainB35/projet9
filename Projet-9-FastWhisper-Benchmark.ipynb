{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4546b15d-ae16-4d3e-b8fc-2b688a8bd8f8",
   "metadata": {},
   "source": [
    "# FasterWhisper benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a94f45a-108b-4886-83b6-99dba4e0535a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "from faster_whisper import WhisperModel\n",
    "from tqdm import tqdm\n",
    "import soundfile as sf  # pour calculer la durÃ©e audio\n",
    "\n",
    "def get_audio_duration(path: Path) -> float:\n",
    "    \"\"\"Retourne la durÃ©e d'un fichier audio en secondes.\"\"\"\n",
    "    try:\n",
    "        f = sf.SoundFile(str(path))\n",
    "        return len(f) / f.samplerate\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def benchmark_models(\n",
    "    input_json: str,\n",
    "    audio_root: str = \"data\",\n",
    "    models: list = [\"small\", \"medium\", \"large\"],\n",
    "    sample_size: int = 1000,\n",
    "    device: str = \"cuda\",\n",
    "    compute_type: str = \"float16\",\n",
    "    beam_size: int = 1,\n",
    "    language: str = \"fr\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Benchmark vitesse d'infÃ©rence pour plusieurs modÃ¨les Whisper.\n",
    "    -> SÃ©quentiel\n",
    "    -> Ã‰chantillon alÃ©atoire de N fichiers\n",
    "    -> Ajoute 'inference_time' et 'audio_duration' dans le JSONL\n",
    "    \"\"\"\n",
    "\n",
    "    # Charger tout le dataset\n",
    "    with open(input_json, \"r\", encoding=\"utf-8\") as fin:\n",
    "        data = json.load(fin)\n",
    "\n",
    "    # Tirer un Ã©chantillon alÃ©atoire\n",
    "    sampled_data = random.sample(data, min(sample_size, len(data)))\n",
    "    print(f\"ðŸŽ¯ Benchmark sur {len(sampled_data)} fichiers audio.\")\n",
    "\n",
    "    for model_size in models:\n",
    "        print(f\"\\nðŸš€ Benchmark du modÃ¨le {model_size}...\")\n",
    "\n",
    "        # Charger le modÃ¨le sur GPU\n",
    "        model = WhisperModel(model_size, device=device, compute_type=compute_type)\n",
    "\n",
    "        output_jsonl = input_json.replace(\".json\", f\"_{model_size}_speed_benchmark.jsonl\")\n",
    "\n",
    "        total_start = time.time()\n",
    "\n",
    "        with open(output_jsonl, \"w\", encoding=\"utf-8\") as fout:\n",
    "            for item in tqdm(sampled_data, desc=f\"Transcription {model_size}\"):\n",
    "                audio_path = Path(audio_root) / item[\"audio_file\"]\n",
    "\n",
    "                audio_duration = get_audio_duration(audio_path)\n",
    "                item[\"audio_duration\"] = audio_duration\n",
    "\n",
    "                if not audio_path.exists():\n",
    "                    item[f\"model_prediction_{model_size}\"] = None\n",
    "                    item[f\"inference_time_{model_size}\"] = None\n",
    "                else:\n",
    "                    try:\n",
    "                        start_time = time.time()\n",
    "                        segments, _ = model.transcribe(\n",
    "                            str(audio_path),\n",
    "                            language=language,\n",
    "                            beam_size=beam_size\n",
    "                        )\n",
    "                        inference_time = time.time() - start_time\n",
    "\n",
    "                        item[f\"model_prediction_{model_size}\"] = \" \".join([seg.text for seg in segments])\n",
    "                        item[f\"inference_time_{model_size}\"] = inference_time\n",
    "                    except Exception as e:\n",
    "                        item[f\"model_prediction_{model_size}\"] = f\"Erreur: {str(e)}\"\n",
    "                        item[f\"inference_time_{model_size}\"] = None\n",
    "\n",
    "                fout.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        elapsed = time.time() - total_start\n",
    "        print(f\"âœ… ModÃ¨le {model_size} terminÃ© en {elapsed/60:.2f} minutes\")\n",
    "        print(f\"ðŸ“‚ RÃ©sultats sauvegardÃ©s dans {output_jsonl}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6aebbedc-6563-4df6-8e4d-e2d14bc9ff28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Benchmark sur 100 fichiers audio.\n",
      "\n",
      "ðŸš€ Benchmark du modÃ¨le small...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transcription small: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:58<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ModÃ¨le small terminÃ© en 0.98 minutes\n",
      "ðŸ“‚ RÃ©sultats sauvegardÃ©s dans data/voxpopuli_fr_train/train_metadata_full_small_speed_benchmark.jsonl\n",
      "\n",
      "ðŸš€ Benchmark du modÃ¨le medium...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transcription medium: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [01:31<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ModÃ¨le medium terminÃ© en 1.52 minutes\n",
      "ðŸ“‚ RÃ©sultats sauvegardÃ©s dans data/voxpopuli_fr_train/train_metadata_full_medium_speed_benchmark.jsonl\n",
      "\n",
      "ðŸš€ Benchmark du modÃ¨le large...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transcription large: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [02:15<00:00,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ModÃ¨le large terminÃ© en 2.26 minutes\n",
      "ðŸ“‚ RÃ©sultats sauvegardÃ©s dans data/voxpopuli_fr_train/train_metadata_full_large_speed_benchmark.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark_models(\n",
    "    input_json=\"data/voxpopuli_fr_train/train_metadata_full.json\",\n",
    "    audio_root=\"data\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8173fd0d-0a5e-45f7-b58d-6df381310b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "from faster_whisper import WhisperModel\n",
    "from tqdm import tqdm\n",
    "import soundfile as sf  # pour calculer la durÃ©e audio\n",
    "\n",
    "def get_audio_duration(path: Path) -> float:\n",
    "    \"\"\"Retourne la durÃ©e d'un fichier audio en secondes.\"\"\"\n",
    "    try:\n",
    "        f = sf.SoundFile(str(path))\n",
    "        return len(f) / f.samplerate\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def benchmark_models(\n",
    "    input_json: str,\n",
    "    audio_root: str = \"data\",\n",
    "    models: list = [\"small\", \"medium\", \"large\"],\n",
    "    sample_size: int = 1000,\n",
    "    device: str = \"cuda\",\n",
    "    compute_type: str = \"float16\",\n",
    "    beam_size: int = 1,\n",
    "    language: str = \"fr\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Benchmark vitesse d'infÃ©rence pour plusieurs modÃ¨les Whisper.\n",
    "    -> SÃ©quentiel\n",
    "    -> Ã‰chantillon alÃ©atoire de N fichiers\n",
    "    -> Ajoute 'inference_time' et 'audio_duration' dans le JSONL\n",
    "    -> Sauvegarde un rÃ©sumÃ© global des temps par modÃ¨le\n",
    "    \"\"\"\n",
    "\n",
    "    # Charger tout le dataset\n",
    "    with open(input_json, \"r\", encoding=\"utf-8\") as fin:\n",
    "        data = json.load(fin)\n",
    "\n",
    "    # Tirer un Ã©chantillon alÃ©atoire\n",
    "    sampled_data = random.sample(data, min(sample_size, len(data)))\n",
    "    print(f\"ðŸŽ¯ Benchmark sur {len(sampled_data)} fichiers audio.\")\n",
    "\n",
    "    summary = {}\n",
    "\n",
    "    for model_size in models:\n",
    "        print(f\"\\nðŸš€ Benchmark du modÃ¨le {model_size}...\")\n",
    "\n",
    "        # Charger le modÃ¨le sur GPU\n",
    "        model = WhisperModel(model_size, device=device, compute_type=compute_type)\n",
    "\n",
    "        output_jsonl = input_json.replace(\".json\", f\"_{model_size}_speed_benchmark.jsonl\")\n",
    "\n",
    "        total_start = time.time()\n",
    "        total_inference_time = 0.0\n",
    "        total_audio_duration = 0.0\n",
    "\n",
    "        with open(output_jsonl, \"w\", encoding=\"utf-8\") as fout:\n",
    "            for item in tqdm(sampled_data, desc=f\"Transcription {model_size}\"):\n",
    "                audio_path = Path(audio_root) / item[\"audio_file\"]\n",
    "\n",
    "                audio_duration = get_audio_duration(audio_path)\n",
    "                item[\"audio_duration\"] = audio_duration\n",
    "                if audio_duration:\n",
    "                    total_audio_duration += audio_duration\n",
    "\n",
    "                if not audio_path.exists():\n",
    "                    item[f\"model_prediction_{model_size}\"] = None\n",
    "                    item[f\"inference_time_{model_size}\"] = None\n",
    "                else:\n",
    "                    try:\n",
    "                        start_time = time.time()\n",
    "                        segments, _ = model.transcribe(\n",
    "                            str(audio_path),\n",
    "                            language=language,\n",
    "                            beam_size=beam_size\n",
    "                        )\n",
    "                        inference_time = time.time() - start_time\n",
    "                        total_inference_time += inference_time\n",
    "\n",
    "                        item[f\"model_prediction_{model_size}\"] = \" \".join([seg.text for seg in segments])\n",
    "                        item[f\"inference_time_{model_size}\"] = inference_time\n",
    "                    except Exception as e:\n",
    "                        item[f\"model_prediction_{model_size}\"] = f\"Erreur: {str(e)}\"\n",
    "                        item[f\"inference_time_{model_size}\"] = None\n",
    "\n",
    "                fout.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        elapsed = time.time() - total_start\n",
    "        print(f\"âœ… ModÃ¨le {model_size} terminÃ© en {elapsed/60:.2f} minutes\")\n",
    "        print(f\"ðŸ“‚ RÃ©sultats sauvegardÃ©s dans {output_jsonl}\")\n",
    "\n",
    "        # Stocker rÃ©sumÃ©\n",
    "        summary[model_size] = {\n",
    "            \"total_wall_time_sec\": elapsed,\n",
    "            \"total_inference_time_sec\": total_inference_time,\n",
    "            \"total_audio_duration_sec\": total_audio_duration,\n",
    "            \"rtf\": total_inference_time / total_audio_duration if total_audio_duration > 0 else None  # Real-Time Factor\n",
    "        }\n",
    "\n",
    "    # Sauvegarder le rÃ©sumÃ© global\n",
    "    summary_file = input_json.replace(\".json\", \"_benchmark_summary.json\")\n",
    "    with open(summary_file, \"w\", encoding=\"utf-8\") as fsum:\n",
    "        json.dump(summary, fsum, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"\\nðŸ“Š RÃ©sumÃ© global sauvegardÃ© dans {summary_file}\")\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd80ff6c-08d4-46b3-8f1c-2569fd7fa5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Benchmark sur 1000 fichiers audio.\n",
      "\n",
      "ðŸš€ Benchmark du modÃ¨le small...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transcription small: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [10:04<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ModÃ¨le small terminÃ© en 10.07 minutes\n",
      "ðŸ“‚ RÃ©sultats sauvegardÃ©s dans data/voxpopuli_fr_train/train_metadata_full_small_speed_benchmark.jsonl\n",
      "\n",
      "ðŸš€ Benchmark du modÃ¨le medium...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transcription medium: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [15:41<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ModÃ¨le medium terminÃ© en 15.70 minutes\n",
      "ðŸ“‚ RÃ©sultats sauvegardÃ©s dans data/voxpopuli_fr_train/train_metadata_full_medium_speed_benchmark.jsonl\n",
      "\n",
      "ðŸš€ Benchmark du modÃ¨le large...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transcription large: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [23:19<00:00,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ModÃ¨le large terminÃ© en 23.32 minutes\n",
      "ðŸ“‚ RÃ©sultats sauvegardÃ©s dans data/voxpopuli_fr_train/train_metadata_full_large_speed_benchmark.jsonl\n",
      "\n",
      "ðŸ“Š RÃ©sumÃ© global sauvegardÃ© dans data/voxpopuli_fr_train/train_metadata_full_benchmark_summary.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'small': {'total_wall_time_sec': 604.2079555988312,\n",
       "  'total_inference_time_sec': 59.06663155555725,\n",
       "  'total_audio_duration_sec': 10258.871874999992,\n",
       "  'rtf': 0.005757614704156474},\n",
       " 'medium': {'total_wall_time_sec': 941.7749347686768,\n",
       "  'total_inference_time_sec': 55.47819924354553,\n",
       "  'total_audio_duration_sec': 10258.871874999992,\n",
       "  'rtf': 0.005407826505635697},\n",
       " 'large': {'total_wall_time_sec': 1399.4844186306,\n",
       "  'total_inference_time_sec': 56.45949172973633,\n",
       "  'total_audio_duration_sec': 10258.871874999992,\n",
       "  'rtf': 0.005503479565557629}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark_models(\n",
    "    input_json=\"data/voxpopuli_fr_train/train_metadata_full.json\",\n",
    "    audio_root=\"data\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95bdb89-cd44-4cf7-80a3-1033b61cceac",
   "metadata": {},
   "source": [
    "# WHisper Medusa "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30125614-bd31-44d1-b4cf-ee27b11b92a5",
   "metadata": {},
   "source": [
    "https://github.com/aiola-lab/whisper-medusa\n",
    "\n",
    "## TÃ©lÃ©charge Miniconda (recommandÃ© pour serveur)\n",
    "wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh\n",
    "bash miniconda.sh -b -p $HOME/miniconda\n",
    "echo \"export PATH=$HOME/miniconda/bin:$PATH\" >> ~/.bashrc\n",
    "source ~/.bashrc\n",
    "\n",
    "conda tos accept --override-channels --channel https://repo.anaconda.com/pkgs/main\n",
    "conda tos accept --override-channels --channel https://repo.anaconda.com/pkgs/r\n",
    "\n",
    "\n",
    "\n",
    "conda create -n whisper-medusa python=3.11 -y\n",
    "conda activate whisper-medusa\n",
    "pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "\n",
    "git clone https://github.com/aiola-lab/whisper-medusa.git\n",
    "cd whisper-medusa\n",
    "pip install -e .\n",
    "\n",
    "\n",
    "# The model is optimized for English audio with sampling rate of 16kHz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcb3533-4234-4efe-86d1-e1aaace1b68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from whisper_medusa import WhisperMedusaModel\n",
    "from transformers import WhisperProcessor\n",
    "\n",
    "def transcribe_medusa_json_sample(\n",
    "    input_json: str,\n",
    "    output_jsonl: str,\n",
    "    audio_root: str = \"data\",\n",
    "    sample_size: int = 10,\n",
    "    model_name: str = \"aiola/whisper-medusa-linear-libri\",\n",
    "    device: str = \"cuda\",\n",
    "    language: str = \"fr\",\n",
    "    regulation_factor: float = 1.01,\n",
    "    regulation_start: int = 140\n",
    "):\n",
    "    \"\"\"\n",
    "    Transcrit un petit Ã©chantillon du dataset JSON avec Whisper-Medusa.\n",
    "    - sample_size dÃ©finit le nombre de fichiers Ã  traiter.\n",
    "    - Ajoute inference_time et audio_duration dans le JSONL.\n",
    "    \"\"\"\n",
    "    # Charger le dataset complet\n",
    "    with open(input_json, \"r\", encoding=\"utf-8\") as fin:\n",
    "        data = json.load(fin)\n",
    "\n",
    "    # Tirer un Ã©chantillon alÃ©atoire\n",
    "    sampled_data = random.sample(data, min(sample_size, len(data)))\n",
    "    print(f\"ðŸŽ¯ Transcription d'un Ã©chantillon de {len(sampled_data)} fichiers audio.\")\n",
    "\n",
    "    # Charger le modÃ¨le et le processor\n",
    "    model = WhisperMedusaModel.from_pretrained(model_name).to(device)\n",
    "    processor = WhisperProcessor.from_pretrained(model_name)\n",
    "\n",
    "    with open(output_jsonl, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for item in tqdm(sampled_data, desc=\"Transcription Medusa\"):\n",
    "            audio_path = Path(audio_root) / item[\"audio_file\"]\n",
    "            if not audio_path.exists():\n",
    "                item[\"model_prediction_medusa\"] = None\n",
    "                item[\"inference_time_medusa\"] = None\n",
    "                fout.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "                continue\n",
    "\n",
    "            # Charger l'audio\n",
    "            waveform, sr = torchaudio.load(audio_path)\n",
    "            if waveform.shape[0] > 1:\n",
    "                waveform = waveform.mean(dim=0, keepdim=True)\n",
    "            SAMPLING_RATE = 16000\n",
    "            if sr != SAMPLING_RATE:\n",
    "                waveform = torchaudio.transforms.Resample(sr, SAMPLING_RATE)(waveform)\n",
    "\n",
    "            item[\"audio_duration\"] = waveform.shape[-1] / SAMPLING_RATE\n",
    "\n",
    "            # PrÃ©parer les features avec attention_mask\n",
    "            input_features = processor(\n",
    "                waveform.squeeze(),\n",
    "                return_tensors=\"pt\",\n",
    "                sampling_rate=SAMPLING_RATE\n",
    "            )\n",
    "            input_features = input_features.to(device)\n",
    "            attention_mask = input_features.attention_mask if hasattr(input_features, \"attention_mask\") else None\n",
    "\n",
    "            # Transcription\n",
    "            start_time = torch.cuda.Event(enable_timing=True)\n",
    "            end_time = torch.cuda.Event(enable_timing=True)\n",
    "            start_time.record()\n",
    "\n",
    "            model_output = model.generate(\n",
    "                input_features.input_features,\n",
    "                attention_mask=attention_mask,\n",
    "                language=language,\n",
    "                exponential_decay_length_penalty=(regulation_start, regulation_factor)\n",
    "            )\n",
    "\n",
    "            end_time.record()\n",
    "            torch.cuda.synchronize()\n",
    "            item[\"inference_time_medusa\"] = start_time.elapsed_time(end_time) / 1000.0  # secondes\n",
    "\n",
    "            predict_ids = model_output[0]\n",
    "            item[\"model_prediction_medusa\"] = processor.decode(predict_ids, skip_special_tokens=True)\n",
    "\n",
    "            fout.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"âœ… RÃ©sultats sauvegardÃ©s dans {output_jsonl}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    transcribe_medusa_json_sample(\n",
    "        input_json=\"/home/projet9/projet9/notebooks/data/voxpopuli_fr_train/train_metadata_full.json\",\n",
    "        output_jsonl=\"/home/projet9/projet9/notebooks/data/voxpopuli_fr_train/train_sample10_medusa.jsonl\",\n",
    "        audio_root=\"/home/projet9/projet9/notebooks/data\",\n",
    "        sample_size=10,\n",
    "        model_name=\"aiola/whisper-medusa-linear-libri\",\n",
    "        device=\"cuda\",\n",
    "        language=\"fr\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6112809-ea3e-4ccd-855f-acca5de4879f",
   "metadata": {},
   "source": [
    "#Â Benchmark Whisper Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7f27bf-4e9f-4859-9666-628db68044fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo apt update\n",
    "sudo apt install ffmpeg -y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "783c5254-be99-4219-8852-f2a6a0cf8710",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "import soundfile as sf\n",
    "from tqdm import tqdm\n",
    "import whisper  # OpenAI Whisper officiel\n",
    "\n",
    "def get_audio_duration(path: Path) -> float:\n",
    "    \"\"\"Retourne la durÃ©e d'un fichier audio en secondes.\"\"\"\n",
    "    try:\n",
    "        with sf.SoundFile(str(path)) as f:\n",
    "            return len(f) / f.samplerate\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def benchmark_whisper(\n",
    "    input_json: str,\n",
    "    audio_root: str = \"data\",\n",
    "    models: list = [\"small\", \"medium\", \"large\"],\n",
    "    sample_size: int = 1000,\n",
    "    device: str = \"cuda\",\n",
    "    beam_size: int = 1,\n",
    "    language: str = \"fr\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Benchmark officiel Whisper (OpenAI) sur un Ã©chantillon de fichiers.\n",
    "    - SÃ©quentiel\n",
    "    - Ajoute 'inference_time' et 'audio_duration'\n",
    "    - Sauvegarde rÃ©sumÃ© global par modÃ¨le\n",
    "    \"\"\"\n",
    "    # Charger dataset\n",
    "    with open(input_json, \"r\", encoding=\"utf-8\") as fin:\n",
    "        data = json.load(fin)\n",
    "\n",
    "    sampled_data = random.sample(data, min(sample_size, len(data)))\n",
    "    print(f\"ðŸŽ¯ Benchmark sur {len(sampled_data)} fichiers audio.\")\n",
    "\n",
    "    summary = {}\n",
    "\n",
    "    for model_size in models:\n",
    "        print(f\"\\nðŸš€ Benchmark du modÃ¨le Whisper {model_size}...\")\n",
    "\n",
    "        model = whisper.load_model(model_size, device=device)\n",
    "\n",
    "        output_jsonl = input_json.replace(\".json\", f\"_{model_size}_speed_benchmark.jsonl\")\n",
    "        total_start = time.time()\n",
    "        total_inference_time = 0.0\n",
    "        total_audio_duration = 0.0\n",
    "\n",
    "        with open(output_jsonl, \"w\", encoding=\"utf-8\") as fout:\n",
    "            for item in tqdm(sampled_data, desc=f\"Transcription {model_size}\"):\n",
    "                audio_path = Path(audio_root) / item[\"audio_file\"]\n",
    "                audio_duration = get_audio_duration(audio_path)\n",
    "                item[\"audio_duration\"] = audio_duration\n",
    "                if audio_duration:\n",
    "                    total_audio_duration += audio_duration\n",
    "\n",
    "                if not audio_path.exists():\n",
    "                    item[f\"model_prediction_{model_size}\"] = None\n",
    "                    item[f\"inference_time_{model_size}\"] = None\n",
    "                else:\n",
    "                    try:\n",
    "                        start_time = time.time()\n",
    "                        result = model.transcribe(str(audio_path), language=language, beam_size=beam_size)\n",
    "                        inference_time = time.time() - start_time\n",
    "                        total_inference_time += inference_time\n",
    "\n",
    "                        item[f\"model_prediction_{model_size}\"] = result[\"text\"]\n",
    "                        item[f\"inference_time_{model_size}\"] = inference_time\n",
    "                    except Exception as e:\n",
    "                        item[f\"model_prediction_{model_size}\"] = f\"Erreur: {str(e)}\"\n",
    "                        item[f\"inference_time_{model_size}\"] = None\n",
    "\n",
    "                fout.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        elapsed = time.time() - total_start\n",
    "        print(f\"âœ… ModÃ¨le {model_size} terminÃ© en {elapsed/60:.2f} minutes\")\n",
    "        print(f\"ðŸ“‚ RÃ©sultats sauvegardÃ©s dans {output_jsonl}\")\n",
    "\n",
    "        summary[model_size] = {\n",
    "            \"total_wall_time_sec\": elapsed,\n",
    "            \"total_inference_time_sec\": total_inference_time,\n",
    "            \"total_audio_duration_sec\": total_audio_duration,\n",
    "            \"rtf\": total_inference_time / total_audio_duration if total_audio_duration > 0 else None\n",
    "        }\n",
    "\n",
    "    # Sauvegarder rÃ©sumÃ© global\n",
    "    summary_file = input_json.replace(\".json\", \"_benchmark_summary.json\")\n",
    "    with open(summary_file, \"w\", encoding=\"utf-8\") as fsum:\n",
    "        json.dump(summary, fsum, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"\\nðŸ“Š RÃ©sumÃ© global sauvegardÃ© dans {summary_file}\")\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "708cc79d-1696-45dd-b6a8-14a486e103bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Benchmark sur 1000 fichiers audio.\n",
      "\n",
      "ðŸš€ Benchmark du modÃ¨le Whisper small...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transcription small: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [10:51<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ModÃ¨le small terminÃ© en 10.86 minutes\n",
      "ðŸ“‚ RÃ©sultats sauvegardÃ©s dans data/voxpopuli_fr_train/train_metadata_full_small_speed_benchmark.jsonl\n",
      "\n",
      "ðŸš€ Benchmark du modÃ¨le Whisper medium...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transcription medium: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [18:34<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ModÃ¨le medium terminÃ© en 18.57 minutes\n",
      "ðŸ“‚ RÃ©sultats sauvegardÃ©s dans data/voxpopuli_fr_train/train_metadata_full_medium_speed_benchmark.jsonl\n",
      "\n",
      "ðŸš€ Benchmark du modÃ¨le Whisper large...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transcription large: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [46:25<00:00,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ModÃ¨le large terminÃ© en 46.43 minutes\n",
      "ðŸ“‚ RÃ©sultats sauvegardÃ©s dans data/voxpopuli_fr_train/train_metadata_full_large_speed_benchmark.jsonl\n",
      "\n",
      "ðŸ“Š RÃ©sumÃ© global sauvegardÃ© dans data/voxpopuli_fr_train/train_metadata_full_benchmark_summary.json\n",
      "RÃ©sumÃ© global : {'small': {'total_wall_time_sec': 651.5620958805084, 'total_inference_time_sec': 649.6017773151398, 'total_audio_duration_sec': 10036.407249999997, 'rtf': 0.06472453350427165}, 'medium': {'total_wall_time_sec': 1114.286206960678, 'total_inference_time_sec': 1113.2970192432404, 'total_audio_duration_sec': 10036.407249999997, 'rtf': 0.11092585140397135}, 'large': {'total_wall_time_sec': 2785.6306076049805, 'total_inference_time_sec': 2784.463607311249, 'total_audio_duration_sec': 10036.407249999997, 'rtf': 0.27743629148879445}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "    summary = benchmark_whisper(\n",
    "        input_json='data/voxpopuli_fr_train/train_metadata_full.json',\n",
    "        audio_root='data',\n",
    "        models=[\"small\", \"medium\", \"large\"],  # les modÃ¨les Whisper Ã  tester\n",
    "        sample_size=1000,                     # Ã©chantillon alÃ©atoire de fichiers\n",
    "        device=\"cuda\",                        # ou \"cpu\"\n",
    "        beam_size=1,                          # beam size pour la transcription\n",
    "        language=\"fr\"                         # franÃ§ais\n",
    "    )\n",
    "\n",
    "    print(\"RÃ©sumÃ© global :\", summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (projet9)",
   "language": "python",
   "name": "projet9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
